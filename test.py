import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

def test_trained_model():
    print("ü§ñ TEST MODEL ƒê√É TRAIN")
    print("="*50)
    
    model_path = "./vietnamese-product-model"
    
    if not os.path.exists(model_path):
        print("‚ùå Kh√¥ng t√¨m th·∫•y model ƒë√£ train!")
        return
    
    # T·∫£i model ƒë√£ train
    print("üîÑ ƒêang t·∫£i model...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )
    model.eval()
    print("‚úÖ ƒê√£ t·∫£i model th√†nh c√¥ng!")
    
    # C√¢u h·ªèi test
    test_questions = [
        "B∆° l·∫°t Anchor gi√° bao nhi√™u?",
        "S·∫£n ph·∫©m B∆° l·∫°t Anchor thu·ªôc danh m·ª•c n√†o?",
        "H·∫°n s·ª≠ d·ª•ng c·ªßa B∆° l·∫°t Anchor l√† g√¨?",
        "B∆° l·∫°t Anchor c√≥ xu·∫•t x·ª© t·ª´ ƒë√¢u?",
        "M√¥ t·∫£ v·ªÅ B∆° l·∫°t Anchor",
        "S·∫£n ph·∫©m n√†y c√≤n bao nhi√™u trong kho?"
    ]
    
    for question in test_questions:
        print(f"\n‚ùì C√¢u h·ªèi: {question}")
        
        # T·∫°o prompt
        input_text = f"H·ªèi: {question}\nƒê√°p:"
        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=128)
        
        # Generate answer
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=200,
                num_return_sequences=1,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=True,
                no_repeat_ngram_size=2,
                early_stopping=True
            )
        
        # Decode k·∫øt qu·∫£
        full_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # T√°ch ph·∫ßn tr·∫£ l·ªùi
        if "ƒê√°p:" in full_answer:
            answer_only = full_answer.split("ƒê√°p:")[-1].strip()
            if "H·ªèi:" in answer_only:
                answer_only = answer_only.split("H·ªèi:")[0].strip()
        else:
            answer_only = full_answer
        
        print(f"‚úÖ Tr·∫£ l·ªùi: {answer_only}")
        print("-" * 80)

def interactive_mode():
    """Ch·∫ø ƒë·ªô h·ªèi ƒë√°p t∆∞∆°ng t√°c"""
    print("\n" + "="*50)
    print("CH·∫æ ƒê·ªò H·ªéI ƒê√ÅP T∆Ø∆†NG T√ÅC")
    print("G√µ 'quit' ƒë·ªÉ tho√°t")
    print("="*50)
    
    model_path = "./vietnamese-product-model"
    
    if not os.path.exists(model_path):
        print("‚ùå Kh√¥ng t√¨m th·∫•y model ƒë√£ train!")
        return
    
    # T·∫£i model
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )
    model.eval()
    
    while True:
        question = input("\nü§î Nh·∫≠p c√¢u h·ªèi: ").strip()
        
        if question.lower() in ['quit', 'exit', 'tho√°t', 'q']:
            print("üëã T·∫°m bi·ªát!")
            break
            
        if not question:
            continue
            
        try:
            input_text = f"H·ªèi: {question}\nƒê√°p:"
            inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=128)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=200,
                    num_return_sequences=1,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id,
                    do_sample=True,
                    no_repeat_ngram_size=2
                )
            
            full_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            if "ƒê√°p:" in full_answer:
                answer_only = full_answer.split("ƒê√°p:")[-1].strip()
                if "H·ªèi:" in answer_only:
                    answer_only = answer_only.split("H·ªèi:")[0].strip()
            else:
                answer_only = full_answer
            
            print(f"ü§ñ Tr·∫£ l·ªùi: {answer_only}")
            
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")

if __name__ == "__main__":
    test_trained_model()
    interactive_mode()