import json
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
import re
import requests
import os
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

class DataLoader:
    def __init__(self, data_path):
        self.data_path = data_path
        self.products = []
        
    def load_data(self):
        """Load d·ªØ li·ªáu t·ª´ file JSON"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                self.products = data
                print(f"‚úÖ ƒê√£ load {len(self.products)} s·∫£n ph·∫©m")
                return self.products, []
            else:
                raise ValueError("C·∫•u tr√∫c JSON kh√¥ng h·ª£p l·ªá")
                
        except Exception as e:
            print(f"‚ùå L·ªói load data: {e}")
            return [], []

import requests
import re
import json
import os
from typing import Dict

class GPTQueryAnalyzer:
    def __init__(self):
        # KH·∫®N C·∫§P: X√ìA API KEY KH·ªéI CODE!
        # S·ª≠ d·ª•ng c√°c c√°ch b·∫£o m·∫≠t b√™n d∆∞·ªõi
        self.api_key = self._get_api_key_safely()
        self.base_url = "https://api.openai.com/v1/chat/completions"
    
    def _get_api_key_safely(self):
        """L·∫•y API key an to√†n - KH√îNG ƒë·ªÉ trong code"""
        # ∆Øu ti√™n 1: Environment variable
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            return api_key
            
        # ∆Øu ti√™n 2: File .env
        try:
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                return api_key
        except ImportError:
            pass
            
        # ∆Øu ti√™n 3: File config.json
        try:
            with open('config.json', 'r') as f:
                config = json.load(f)
                api_key = config.get('OPENAI_API_KEY')
                if api_key:
                    return api_key
        except:
            pass
            
        # ∆Øu ti√™n 4: Nh·∫≠p t·ª´ ng∆∞·ªùi d√πng
        print("üîë Kh√¥ng t√¨m th·∫•y OpenAI API Key")
        api_key = input("Nh·∫≠p API key c·ªßa b·∫°n (b·∫Øt ƒë·∫ßu v·ªõi sk-): ").strip()
        
        if api_key and api_key.startswith('sk-'):
            self._save_api_key(api_key)
            return api_key
        else:
            print("‚ö†Ô∏è API Key kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng fallback mode.")
            return None
    
    def _save_api_key(self, api_key):
        """L∆∞u API key v√†o file .env"""
        try:
            with open('.env', 'w') as f:
                f.write(f'OPENAI_API_KEY={api_key}\n')
            print("‚úÖ ƒê√£ l∆∞u API key v√†o file .env")
            
            # T·∫°o file .gitignore ƒë·ªÉ tr√°nh commit nh·∫ßm
            if not os.path.exists('.gitignore'):
                with open('.gitignore', 'w') as f:
                    f.write('.env\nconfig.json\n__pycache__/\n*.pyc\n')
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u API key: {e}")

    def analyze_query(self, user_query: str) -> Dict:
        """Ph√¢n t√≠ch query v·ªõi GPT"""
        if not self.api_key:
            print("üîß S·ª≠ d·ª•ng fallback mode (kh√¥ng c√≥ API key)")
            return self._fallback_analysis(user_query)
            
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI cho si√™u th·ªã. Ph√¢n t√≠ch query v√† tr·∫£ v·ªÅ JSON:
        {
            "original_query": "C√¢u h·ªèi g·ªëc",
            "processed_query": "T·ª´ kh√≥a t√¨m ki·∫øm",
            "intent": "search | recommendation | compare | budget",
            "category": "Danh m·ª•c s·∫£n ph·∫©m",
            "brand": "Th∆∞∆°ng hi·ªáu",
            "attributes": {
                "price_range": "low | medium | high | any",
                "purpose": "cooking | drinking | cleaning | personal_care | gift"
            }
        }"""
        
        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            "temperature": 0.1,
            "max_tokens": 150
        }
        
        try:
            print("üîÑ ƒêang ph√¢n t√≠ch v·ªõi GPT...")
            response = requests.post(
                self.base_url, 
                headers=headers, 
                json=payload, 
                timeout=30  # TƒÉng timeout
            )
            
            # X·ª≠ l√Ω c√°c m√£ l·ªói ph·ªï bi·∫øn
            if response.status_code == 401:
                print("‚ùå API Key kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n")
                return self._fallback_analysis(user_query)
            elif response.status_code == 429:
                print("‚ùå Qu√° gi·ªõi h·∫°n rate limit - th·ª≠ l·∫°i sau 60 gi√¢y")
                return self._fallback_analysis(user_query)
            elif response.status_code == 403:
                print("‚ùå T√†i kho·∫£n b·ªã h·∫°n ch·∫ø ho·∫∑c h·∫øt credit")
                return self._fallback_analysis(user_query)
            elif response.status_code == 500:
                print("‚ùå L·ªói server OpenAI - th·ª≠ l·∫°i sau")
                return self._fallback_analysis(user_query)
            elif response.status_code != 200:
                print(f"‚ùå L·ªói API: {response.status_code} - {response.text}")
                return self._fallback_analysis(user_query)
                
            response.raise_for_status()
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            # Parse JSON t·ª´ response
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())
                
                # T√≠nh cost ∆∞·ªõc t√≠nh
                usage = result.get('usage', {})
                total_tokens = usage.get('total_tokens', 0)
                cost = (total_tokens / 1000) * 0.002  # ~$0.002 per 1K tokens
                print(f"üí∞ Token usage: {total_tokens} (~${cost:.6f})")
                
                return self._validate_analysis(analysis, user_query)
            else:
                print("‚ùå Kh√¥ng th·ªÉ parse JSON t·ª´ GPT response")
                return self._fallback_analysis(user_query)
                
        except requests.exceptions.Timeout:
            print("‚ùå GPT request timeout sau 30 gi√¢y")
            return self._fallback_analysis(user_query)
        except requests.exceptions.ConnectionError:
            print("‚ùå L·ªói k·∫øt n·ªëi - ki·ªÉm tra internet")
            return self._fallback_analysis(user_query)
        except requests.exceptions.RequestException as e:
            print(f"‚ùå L·ªói request: {e}")
            return self._fallback_analysis(user_query)
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
            return self._fallback_analysis(user_query)
    
    def _validate_analysis(self, analysis: Dict, original_query: str) -> Dict:
        """Validate analysis result"""
        # ƒê·∫£m b·∫£o processed_query kh√¥ng r·ªóng
        if not analysis.get('processed_query') or analysis['processed_query'].strip() == '':
            analysis['processed_query'] = self._simple_preprocess(original_query)
        
        # ƒê·∫£m b·∫£o c√°c tr∆∞·ªùng c·∫ßn thi·∫øt t·ªìn t·∫°i
        default_analysis = {
            "original_query": original_query,
            "processed_query": self._simple_preprocess(original_query),
            "intent": "search",
            "category": None,
            "brand": None,
            "attributes": {
                "price_range": "any",
                "purpose": "daily_use"
            }
        }
        
        # Merge v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
        for key, default_value in default_analysis.items():
            if key not in analysis:
                analysis[key] = default_value
            elif isinstance(default_value, dict) and isinstance(analysis[key], dict):
                # Merge nested dictionaries
                for sub_key, sub_default in default_value.items():
                    if sub_key not in analysis[key]:
                        analysis[key][sub_key] = sub_default
        
        return analysis
    
    def _fallback_analysis(self, user_query: str) -> Dict:
        """Fallback analysis khi GPT kh√¥ng ho·∫°t ƒë·ªông"""
        query_lower = user_query.lower()
        
        # Ph√¢n t√≠ch intent ƒë∆°n gi·∫£n
        intent = "search"
        if any(word in query_lower for word in ['g·ª£i √Ω', 'n√™n mua', 'khuy√™n', 'recommend']):
            intent = "recommendation"
        elif any(word in query_lower for word in ['so s√°nh', 'compare']):
            intent = "compare"
        elif any(word in query_lower for word in ['r·∫ª', 'gi√°', 'ti·∫øt ki·ªám', 'budget']):
            intent = "budget"
        
        # Ph√¢n t√≠ch category ƒë∆°n gi·∫£n
        category = None
        category_keywords = {
            "Th·ª±c ph·∫©m": ['g·∫°o', 'c∆°m', 'm√¨', 'b√∫n', 'ph·ªü', 'th·ªãt', 'c√°', 'rau', 'tr√°i c√¢y'],
            "ƒê·ªì u·ªëng": ['n∆∞·ªõc', 'bia', 'r∆∞·ª£u', 's·ªØa', 'cafe', 'tr√†', 'n∆∞·ªõc ng·ªçt'],
            "Gia d·ª•ng": ['b√†n', 'gh·∫ø', 't·ªß', 'b·∫øp', 'ƒë√®n', 'ch√©n', 'dƒ©a', 'b√°t'],
            "V·ªá sinh": ['gi·∫∑t', 't·∫©y', 'x√† ph√≤ng', 'd·∫ßu g·ªôi', 's·ªØa t·∫Øm', 'n∆∞·ªõc r·ª≠a'],
            "ƒêi·ªán t·ª≠": ['ƒëi·ªán tho·∫°i', 'tivi', 't·ªß l·∫°nh', 'm√°y gi·∫∑t']
        }
        
        for cat, keywords in category_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                category = cat
                break
        
        # Ph√¢n t√≠ch brand ƒë∆°n gi·∫£n
        brand = None
        common_brands = ['vinamilk', 'cocacola', 'pepsi', 'tiger', 'heineken', 'panasonic']
        for brand_name in common_brands:
            if brand_name in query_lower:
                brand = brand_name
                break
        
        # Ph√¢n t√≠ch price range
        price_range = "any"
        if 'r·∫ª' in query_lower or 'gi√° r·∫ª' in query_lower:
            price_range = "low"
        elif 'ƒë·∫Øt' in query_lower or 'cao c·∫•p' in query_lower:
            price_range = "high"
        
        return {
            "original_query": user_query,
            "processed_query": self._simple_preprocess(user_query),
            "intent": intent,
            "category": category,
            "brand": brand,
            "attributes": {
                "price_range": price_range,
                "purpose": "daily_use"
            }
        }
    
    def _simple_preprocess(self, query: str) -> str:
        """Ti·ªÅn x·ª≠ l√Ω query ƒë∆°n gi·∫£n"""
        if not query:
            return ""
            
        stopwords = ['t√¥i', 'mu·ªën', 'mua', 'c·∫ßn', 't√¨m', 'c√≥', 'n√†o', '·∫°', '∆°i', 'cho']
        query = re.sub(r'[^\w\s]', ' ', query.lower())
        words = [word for word in query.split() if word not in stopwords and len(word) > 1]
        return ' '.join(words)
class SmartProductFilter:
    def __init__(self, products):
        self.products = products
        
    def apply_filters(self, products: List[Dict], analysis: Dict) -> List[Dict]:
        """√Åp d·ª•ng c√°c b·ªô l·ªçc th√¥ng minh"""
        filtered_products = products.copy()
        
        # L·ªçc theo category
        if analysis["category"]:
            filtered_products = self._filter_by_category(filtered_products, analysis["category"])
        
        # L·ªçc theo brand
        if analysis["brand"]:
            filtered_products = self._filter_by_brand(filtered_products, analysis["brand"])
        
        # L·ªçc theo price range
        price_range = analysis["attributes"]["price_range"]
        if price_range != "any":
            filtered_products = self._filter_by_price(filtered_products, price_range)
        
        return filtered_products
    
    def _filter_by_category(self, products: List[Dict], category: str) -> List[Dict]:
        """L·ªçc s·∫£n ph·∫©m theo category"""
        category_lower = category.lower()
        filtered = []
        for product in products:
            product_category = product.get('category', '').lower()
            product_name = product.get('name', '').lower()
            
            if (category_lower in product_category or 
                category_lower in product_name):
                filtered.append(product)
        return filtered
    
    def _filter_by_brand(self, products: List[Dict], brand: str) -> List[Dict]:
        """L·ªçc s·∫£n ph·∫©m theo brand"""
        brand_lower = brand.lower()
        filtered = []
        for product in products:
            product_name = product.get('name', '').lower()
            product_desc = product.get('description', '').lower()
            
            if brand_lower in product_name or brand_lower in product_desc:
                filtered.append(product)
        return filtered
    
    def _filter_by_price(self, products: List[Dict], price_range: str) -> List[Dict]:
        """L·ªçc s·∫£n ph·∫©m theo kho·∫£ng gi√°"""
        price_mapping = {
            "low": (0, 50000),
            "medium": (50000, 200000),
            "high": (200000, float('inf'))
        }
        
        if price_range in price_mapping:
            min_price, max_price = price_mapping[price_range]
            filtered = []
            for product in products:
                price = product.get('price', 0)
                if min_price <= price <= max_price:
                    filtered.append(product)
            return filtered
        return products

class EnhancedRecommender:
    def __init__(self, products):
        self.products = products
        self.tfidf_vectorizer = TfidfVectorizer(max_features=500)
        self.product_filter = SmartProductFilter(products)
        self.product_features = None
        self._prepare_product_data()
        
    def _prepare_product_data(self):
        """Chu·∫©n b·ªã d·ªØ li·ªáu s·∫£n ph·∫©m"""
        self.product_texts = []
        for product in self.products:
            text_data = f"{product.get('name', '')} {product.get('category', '')} {product.get('description', '')} {product.get('content', '')}"
            self.product_texts.append(text_data)
    
    def extract_features(self):
        """Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng"""
        if not self.product_texts:
            self._prepare_product_data()
            
        try:
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.product_texts)
            n_components = min(50, len(self.products)-1)
            
            if n_components > 0:
                self.svd = TruncatedSVD(n_components=n_components)
                self.product_features = self.svd.fit_transform(tfidf_matrix)
            else:
                self.product_features = tfidf_matrix.toarray()
                
        except Exception as e:
            print(f"‚ùå L·ªói extract features: {e}")
            # Fallback: random features
            self.product_features = np.random.rand(len(self.products), 50)
    
    def recommend(self, analysis_result: Dict, top_k: int = 10) -> List[Dict]:
        """G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n ph√¢n t√≠ch chi ti·∫øt"""
        if self.product_features is None:
            self.extract_features()
            
        # B∆∞·ªõc 1: T√¨m ki·∫øm c∆° b·∫£n d·ª±a tr√™n processed_query
        query = analysis_result["processed_query"]
        if query and query.strip() != "":
            try:
                query_vec = self.tfidf_vectorizer.transform([query])
                
                if hasattr(self, 'svd'):
                    query_vec_reduced = self.svd.transform(query_vec)
                    similarities = cosine_similarity(query_vec_reduced, self.product_features)[0]
                else:
                    similarities = cosine_similarity(query_vec, self.product_features)[0]
                
                # K·∫øt h·ª£p s·∫£n ph·∫©m v√† similarity scores
                scored_products = []
                for idx, similarity in enumerate(similarities):
                    if similarity > 0.01:  # Ng∆∞·ª°ng t·ªëi thi·ªÉu
                        product = self.products[idx].copy()
                        product['similarity_score'] = float(similarity)
                        scored_products.append(product)
                
                # S·∫Øp x·∫øp theo similarity
                scored_products.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
                
            except Exception as e:
                print(f"‚ùå L·ªói t√¨m ki·∫øm: {e}")
                scored_products = self.products.copy()
        else:
            scored_products = self.products.copy()
        
        # B∆∞·ªõc 2: √Åp d·ª•ng c√°c b·ªô l·ªçc th√¥ng minh
        filtered_products = self.product_filter.apply_filters(scored_products, analysis_result)
        
        # B∆∞·ªõc 3: ∆Øu ti√™n theo intent
        final_results = self._prioritize_by_intent(filtered_products, analysis_result["intent"])
        
        return final_results[:top_k]
    
    def _prioritize_by_intent(self, products: List[Dict], intent: str) -> List[Dict]:
        """∆Øu ti√™n s·∫£n ph·∫©m d·ª±a tr√™n √Ω ƒë·ªãnh"""
        if intent == "budget":
            # ∆Øu ti√™n gi√° r·∫ª
            return sorted(products, key=lambda x: x.get('price', float('inf')))
        elif intent == "compare":
            # Gi·ªØ nguy√™n th·ª© t·ª± similarity
            return products
        else:
            # M·∫∑c ƒë·ªãnh: similarity score
            return sorted(products, key=lambda x: x.get('similarity_score', 0), reverse=True)

class SupermarketAI:
    def __init__(self, data_path):
        self.data_path = data_path
        self.data_loader = DataLoader(data_path)  # ƒê√É ƒê∆Ø·ª¢C ƒê·ªäNH NGHƒ®A
        self.gpt_analyzer = GPTQueryAnalyzer()
        self.recommender = None
        
    def initialize_system(self):
        """Kh·ªüi t·∫°o h·ªá th·ªëng"""
        print("üîÑ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng AI th√¥ng minh...")
        
        # Load d·ªØ li·ªáu
        products, _ = self.data_loader.load_data()
        
        if not products:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m")
            return False
            
        # Kh·ªüi t·∫°o recommender
        self.recommender = EnhancedRecommender(products)
        
        # Train model
        print("üîÑ Training AI model...")
        self.recommender.extract_features()
        
        print("‚úÖ H·ªá th·ªëng AI th√¥ng minh ƒë√£ s·∫µn s√†ng!")
        return True
        
    def process_query(self, user_query: str, top_k: int = 5) -> Dict:
        """X·ª≠ l√Ω query ng∆∞·ªùi d√πng v·ªõi GPT"""
        if not self.recommender:
            print("‚ùå H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            return None
        
        # Ph√¢n t√≠ch query v·ªõi GPT
        analysis = self.gpt_analyzer.analyze_query(user_query)
        
        print(f"\nüîç PH√ÇN T√çCH GPT:")
        print(f"   Query g·ªëc: '{analysis['original_query']}'")
        print(f"   Query x·ª≠ l√Ω: '{analysis['processed_query']}'")
        print(f"   √ù ƒë·ªãnh: {analysis['intent']}")
        print(f"   Danh m·ª•c: {analysis['category']}")
        print(f"   Th∆∞∆°ng hi·ªáu: {analysis['brand']}")
        print(f"   Thu·ªôc t√≠nh: {analysis['attributes']}")
        
        # L·∫•y recommendations
        recommendations = self.recommender.recommend(analysis, top_k)
        
        return {
            'analysis': analysis,
            'recommendations': recommendations,
            'total_results': len(recommendations)
        }
    
    def display_recommendations(self, result):
        """Hi·ªÉn th·ªã k·∫øt qu·∫£ recommendations"""
        if not result or not result['recommendations']:
            print("‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p")
            return
            
        analysis = result['analysis']
        
        print(f"\nüéØ K·∫æT QU·∫¢ CHO: '{analysis['original_query']}'")
        print(f"üìä T√¨m th·∫•y {result['total_results']} s·∫£n ph·∫©m ph√π h·ª£p")
        print("=" * 80)
        
        for i, product in enumerate(result['recommendations'], 1):
            print(f"{i}. {product['name']}")
            print(f"   üìÇ Danh m·ª•c: {product.get('category', 'N/A')}")
            print(f"   üí∞ Gi√°: {product.get('price', 'N/A'):,} VND")
            print(f"   üì¶ T·ªìn kho: {product.get('quantity', 'N/A')}")
            
            # Hi·ªÉn th·ªã c√°c scores n·∫øu c√≥
            if 'similarity_score' in product:
                print(f"   üîç ƒê·ªô ph√π h·ª£p: {product['similarity_score']:.3f}")
                
            print("-" * 50)

# H√†m main ƒë·ªÉ test
def main():
    data_path = "data.json"  # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n ƒë·∫øn file data c·ªßa b·∫°n
    
    # Kh·ªüi t·∫°o h·ªá th·ªëng
    ai_system = SupermarketAI(data_path)
    
    if not ai_system.initialize_system():
        return
    
    # Test c√°c query ƒëa d·∫°ng
    test_queries = [
        "T√¥i mu·ªën mua b∆° l·∫°t Anchor gi√° r·∫ª",
    ]
    
    print("\n" + "="*80)
    print("üß™ TEST H·ªÜ TH·ªêNG AI TH√îNG MINH V·ªöI GPT")
    print("="*80)
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"üß™ TEST: '{query}'")
        print('='*50)
        
        result = ai_system.process_query(query, top_k=3)
        ai_system.display_recommendations(result)

if __name__ == "__main__":
    main()